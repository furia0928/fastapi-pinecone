import os
import time
import json
import logging
import sys
from uuid import uuid4
from langchain_openai import OpenAIEmbeddings
from pinecone import Pinecone, ServerlessSpec
from dotenv import load_dotenv

load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(stream=sys.stdout, level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# ğŸ“Œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your-openai-api-key")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY", "your-pinecone-api-key")

# ğŸ“Œ Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
pc = Pinecone(api_key=PINECONE_API_KEY)

# ğŸ“Œ ì¸ë±ìŠ¤ ì„¤ì •
INDEX_NAME = "markdown-index"
NAMESPACE = "langchain_api"

# ğŸ“Œ Pinecone ì„œë²„ë¦¬ìŠ¤ ì¸ë±ìŠ¤ ìƒì„±
if INDEX_NAME not in [i.name for i in pc.list_indexes()]:
    pc.create_index(
        name=INDEX_NAME,
        dimension=1536,  # OpenAI ì„ë² ë”© ë²¡í„° í¬ê¸°
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

# ğŸ“Œ Pinecone ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°
index = pc.Index(INDEX_NAME)

# ğŸ“Œ Markdown íŒŒì¼ í´ë” ë° ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì„¤ì •
FOLDER_PATH = "./markdown_outputs"
CHECKPOINT_FILE = "checkpoint.json"
BATCH_SIZE = 50  # í•œ ë²ˆì— ì—…ì„œíŠ¸í•  ë²¡í„° ê°œìˆ˜

# ğŸ“Œ ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë¡œë“œ ë° ì €ì¥ í•¨ìˆ˜
def load_checkpoint():
    if os.path.exists(CHECKPOINT_FILE):
        with open(CHECKPOINT_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"processed_files": []}

def save_checkpoint(data):
    with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

checkpoint = load_checkpoint()

# ğŸ“Œ OpenAI ì„ë² ë”© ìƒì„± í•¨ìˆ˜ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
def get_embedding(text, retry=3):
    """í…ìŠ¤íŠ¸ë¥¼ OpenAI ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„)"""
    embedding_model = OpenAIEmbeddings(
        model="text-embedding-ada-002",
        api_key=OPENAI_API_KEY
    )
    attempts = 0
    while attempts < retry:
        try:
            return embedding_model.embed_query(text)
        except Exception as e:
            attempts += 1
            logging.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (ì¬ì‹œë„ {attempts}/{retry}): {e}")
            time.sleep(2)
    raise Exception("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")

# ğŸ“Œ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜
def chunk_text(text, chunk_size=512, overlap=50):
    """í…ìŠ¤íŠ¸ë¥¼ ì¼ì • í¬ê¸°ë¡œ ë‚˜ëˆ„ê³ , ê²¹ì¹˜ëŠ” ë¶€ë¶„ ìœ ì§€"""
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunks.append(text[i : i + chunk_size])
    return chunks

# ğŸ“Œ ë°°ì¹˜ ì—…ì„œíŠ¸ í•¨ìˆ˜ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
def upsert_batch(vectors, retry=3):
    attempts = 0
    while attempts < retry:
        try:
            index.upsert(vectors=vectors, namespace=NAMESPACE)
            logging.info(f"âœ… {len(vectors)}ê°œ ë²¡í„° ì €ì¥ ì™„ë£Œ!")
            return
        except Exception as e:
            attempts += 1
            logging.error(f"ë°°ì¹˜ ì—…ì„œíŠ¸ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempts}/{retry}): {e}")
            time.sleep(2)
    raise Exception("ë°°ì¹˜ ì—…ì„œíŠ¸ ì‹¤íŒ¨: ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")

# ğŸ“Œ Markdown íŒŒì¼ì„ Pineconeì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜ (ì²´í¬í¬ì¸íŠ¸ ë° ì—ëŸ¬ í•¸ë“¤ë§ ì ìš©)
def process_markdown_files(folder_path):
    all_vectors = []
    processed_files = set(checkpoint.get("processed_files", []))

    for filename in os.listdir(folder_path):
        if not filename.endswith(".md"):
            continue
        if filename in processed_files:
            logging.info(f"ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ìŠ¤í‚µ: {filename}")
            continue

        file_path = os.path.join(folder_path, filename)
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()
        except Exception as e:
            logging.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {filename} - {e}")
            continue

        # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = chunk_text(text)
        logging.info(f"{filename}: {len(chunks)} ì²­í¬ ìƒì„±ë¨.")

        for i, chunk in enumerate(chunks):
            try:
                embedding = get_embedding(chunk)
            except Exception as e:
                logging.error(f"{filename} íŒŒì¼ì˜ ì²­í¬ {i} ì„ë² ë”© ì‹¤íŒ¨: {e}")
                continue  # í•´ë‹¹ ì²­í¬ë§Œ ê±´ë„ˆë›°ê¸°

            vector_id = f"{filename}_{i}"
            metadata = {"filename": filename, "chunk_id": i, "text": chunk}
            all_vectors.append((vector_id, embedding, metadata))

            # BATCH ì—…ì„œíŠ¸
            if len(all_vectors) >= BATCH_SIZE:
                try:
                    upsert_batch(all_vectors)
                except Exception as e:
                    logging.error(f"ë°°ì¹˜ ì—…ì„œíŠ¸ ìµœì¢… ì‹¤íŒ¨: {e}")
                    # ì‹¤íŒ¨í•œ ë°°ì¹˜ëŠ” ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•˜ë„ë¡ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
                all_vectors = []

        # íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
        checkpoint["processed_files"].append(filename)
        save_checkpoint(checkpoint)
        logging.info(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ ë° ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {filename}")

    # ë‚¨ì€ ë²¡í„° ì—…ì„œíŠ¸
    if all_vectors:
        try:
            upsert_batch(all_vectors)
        except Exception as e:
            logging.error(f"ë§ˆì§€ë§‰ ë°°ì¹˜ ì—…ì„œíŠ¸ ì‹¤íŒ¨: {e}")

# ğŸ“Œ ì‹¤í–‰
process_markdown_files(FOLDER_PATH)

# ğŸ“Œ ë²¡í„° ì €ì¥ í™•ì¸
time.sleep(5)  # Pinecone ë°˜ì˜ ëŒ€ê¸°
try:
    stats = index.describe_index_stats()
    logging.info(f"Pinecone ì¸ë±ìŠ¤ ìƒíƒœ: {stats}")
except Exception as e:
    logging.error(f"ì¸ë±ìŠ¤ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")

logging.info("ğŸ¯ ëª¨ë“  Markdown íŒŒì¼ì´ Pineconeì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
